#!/usr/bin/env python

# Copyright 2016 TensorHub, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Port style application providing TensorFlow support

This is not a true Erlang external port but is used in a similar way,
taking requests from stdin and responding over stdout. The protocol is
binary and described below.

A requests is:

  REQ = REF + CMD + ARG-COUNT + ARGS
  REF = uint (4)
  CMD = ushort (2)
  ARG-COUNT = uint (4)
  ARGS = ARG*
  ARG = ARG-LEN + ARG-VAL
  ARG-LEN = ulong (8)
  ARG_VAL = bytes (ARG-LEN)

A response is:

  RESP = REF + STATUS + PART-COUNT + PARTS
  REF = uint (4)
  STATUS = ushort (2)
  PART-COUNT = uint (4)
  PARTS = PART*
  PART = PART-LEN + PART-VAL
  PART-LEN = ulong (8)
  PART-VAL = bytes (PART-LEN)

"""
from __future__ import division

import glob
import imghdr
import json
import os
import re
import struct
import sys
import time

STDIN = sys.stdin
STDOUT = sys.stdout
STDERR = sys.stderr

# ===================================================================
# Protocol
# ===================================================================

class ErrorResponse(BaseException):

    def __init__(self, req, msg):
        self.req = req
        self.msg = msg

class ProtocolError(Exception):
    pass

class Request(object):

    def __init__(self, ref, cmd, args):
        self.ref = ref
        self.cmd = cmd
        self.args = args

def read_request():
    header_bytes = STDIN.read(10)
    if not header_bytes:
        return None
    ref, cmd, arg_count = struct.unpack(">IHI", header_bytes)
    args = []
    for _ in range(arg_count):
        arg_len, = struct.unpack(">Q", STDIN.read(8))
        args.append(STDIN.read(arg_len))
    return Request(ref, cmd, args)

def send_response(req, status, parts):
    STDOUT.write(struct.pack(">IHI", req.ref, status, len(parts)))
    for part in parts:
        STDOUT.write(struct.pack(">Q", len(part)))
        STDOUT.write(part)
    STDOUT.flush()

STATUS_OK = 0
STATUS_ERROR = 1

def send_ok_response(req, parts=[]):
    send_response(req, STATUS_OK, parts)

def send_error_response(req, parts):
    send_response(req, STATUS_ERROR, parts)

def not_found_response():
    return ["not_found"]

# ===================================================================
# Request cmd dispatch
# ===================================================================

CMD_TEST_PROTOCOL        = 0
CMD_READ_IMAGE           = 1
CMD_LOAD_MODEL           = 2
CMD_RUN_MODEL            = 3
CMD_MODEL_INFO           = 4
CMD_MODEL_STATS          = 5
CMD_RUN_MODEL_WITH_STATS = 6

def handle_request(req):
    if req.cmd == CMD_READ_IMAGE:
        handle_read_image(req)
    elif req.cmd == CMD_LOAD_MODEL:
        handle_load_model(req)
    elif req.cmd == CMD_RUN_MODEL:
        handle_run_model(req)
    elif req.cmd == CMD_RUN_MODEL_WITH_STATS:
        handle_run_model_with_stats(req)
    elif req.cmd == CMD_MODEL_INFO:
        handle_model_info(req)
    elif req.cmd == CMD_MODEL_STATS:
        handle_model_stats(req)
    elif req.cmd == CMD_TEST_PROTOCOL:
        handle_test_protocol(req)
    else:
        raise ProtocolError("cmd: %r" + req.cmd)

# ===================================================================
# Test protocol
# ===================================================================

def handle_test_protocol(req):
    send_ok_response(req, req.args)

# ===================================================================
# Read image
# ===================================================================

class Image(object):

    def __init__(self, event_file, tag, tf_image):
        self.event_file = event_file
        self.tag = tag
        self.height = tf_image.height
        self.width = tf_image.width
        self.colorspace = tf_image.colorspace
        self.image_bytes = tf_image.encoded_image_string

def handle_read_image(req):
    path, index = validate_read_image_args(req)
    cur = 0
    for image in iter_images(path):
        if cur == index:
            send_ok_response(req, image_response(image))
            break
        else:
            cur += 1
    else:
        raise ErrorResponse(req, "not found")

def validate_read_image_args(req):
    if len(req.args) != 2:
        raise ProtocolError("read-image requires 2 arguments")
    path = req.args[0]
    if not os.path.exists(path):
        raise ErrorResponse(req, "not found")
    index = int(req.args[1])
    if index < 0:
        raise ErrorResponse(req, "not found")
    return path, index

def iter_images(path):
    from tensorflow.tensorboard.backend.event_processing \
        import event_file_loader
    for event_file in find_event_files(path):
        loader = event_file_loader.EventFileLoader(event_file)
        for event in loader.Load():
            if event.HasField("summary"):
                for value in event.summary.value:
                    if value.HasField("image"):
                        yield Image(event_file, value.tag, value.image)

def find_event_files(root_dir):
    pattern = "*.tfevents*"
    for path in glob.iglob(os.path.join(root_dir, pattern)):
        yield path
    for path in glob.iglob(os.path.join(root_dir, "**", pattern)):
        yield path

def image_response(image):
    return [
        image.event_file,
        image.tag,
        encode_image_dimensions(image.height, image.width, image.colorspace),
        encode_image_type(image.image_bytes),
        image.image_bytes
    ]

def encode_image_dimensions(h, w, d):
    return "%i %i %i" % (h, w, d)

def encode_image_type(image_bytes):
    guessed = imghdr.what(None, image_bytes)
    return guessed if guessed else ""

# ===================================================================
# Model support
# ===================================================================

class ModelError(Exception):
    pass

class ModelStats(object):

    N = 5 # observations used for weighted averages

    def __init__(self):
        self.reset()

    def reset(self):
        self._batches = []

    def update(self, run_count, time, avg_memory):
        self._batches.append((run_count, time, avg_memory))

    def generate(self):
        return {
            "last_batch_time_ms": self._last_batch_time_ms(),
            "average_batch_time_ms": self._average_batch_time_ms(self.N),
            "predictions_per_second": self._predictions_per_second(self.N),
            "last_memory_bytes": self._last_memory_bytes()
        }

    def _last_batch_time_ms(self):
        if self._batches:
            return self._batches[-1][1] / 1000
        else:
            return None

    def _average_batch_time_ms(self, n0):
        if self._batches:
            # Weighted average
            n = min(n0, len(self._batches))
            sample = self._batches[-n:]
            weighted_total = 0
            for i, val in enumerate(sample):
                weighted_total += (i + 1) * val[1] / 1000
            return weighted_total / (n * (n + 1) / 2)
        else:
            return None

    def _predictions_per_second(self, n0):
        if self._batches:
            # Weighted predictions
            n = min(n0, len(self._batches))
            sample = self._batches[-n:]
            weighted_predictions = 0
            weighted_seconds = 0
            for i, (predictions, time_us, _avg_mem) in enumerate(sample):
                weighted_predictions += (i + 1) * predictions
                weighted_seconds += (i + 1) * (time_us / 1000000)
            if weighted_seconds > 0:
                return weighted_predictions / weighted_seconds
            else:
                return None
        else:
            return None

    def _last_memory_bytes(self):
        if self._batches:
            return self._batches[-1][2]
        else:
            return None

def init_session(vars_path):
    import tensorflow as tf
    graph = tf.Graph()
    with graph.as_default():
        saver = tf.train.import_meta_graph(
            vars_path + ".meta",
            clear_devices=True)
    config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))
    session = tf.Session(config=config, graph=graph)
    saver.restore(session, vars_path)
    return session

def graph_outputs(graph):
    tensor_map = {}
    for val in graph.get_collection("outputs"):
        tensor_map.update(json.loads(val))
    keys = tensor_map.keys()
    def tensor_for_key(key):
        try:
            return graph.get_tensor_by_name(tensor_map[key])
        except ValueError, e:
            raise ModelError("Invalid output for '%s': %s" % (key, e))
    tensors = [tensor_for_key(key) for key in keys]
    return tensors, keys

def graph_inputs(graph):
    tensor_map = {}
    for val in graph.get_collection("inputs"):
        tensor_map.update(json.loads(val))
    def tensor_for_key(key):
        try:
            return graph.get_tensor_by_name(tensor_map[key])
        except ValueError, e:
            raise ModelError("Invalid input for '%s': %s" % (key, e))
    return {
        key: tensor_for_key(key) for key in tensor_map
    }

def tensor_info(tensor):
    return {
        "tensor": tensor.name,
        "dtype": tensor.dtype.name,
        "shape": str(tensor.get_shape())
    }

class Model(object):

    def __init__(self):
        self._serving_path = None
        self._session = None
        self._output_tensors = []
        self._output_keys = []
        self._input_tensor_map = {}
        self._stats = ModelStats()

    def ensure_serving_path(self, path):
        if self._serving_path != path:
            self._reset_for_path(path)

    def _reset_for_path(self, path):
        self._serving_path = None
        if self._session:
            self._session.close()
        self._stats.reset()
        session = init_session(path)
        self._session = session
        output_tensors, output_keys = graph_outputs(session.graph)
        self._output_tensors = output_tensors
        self._output_keys = output_keys
        self._input_tensor_map = graph_inputs(session.graph)
        self._serving_path = path

    def run(self, instances, trace_stats=False):
        import tensorflow as tf
        inputs = self._inputs_feed_dict(instances)
        try:
            run_result = self._session_run(inputs, trace_stats)
        except tf.errors.InvalidArgumentError, e:
            raise ModelError(e.message)
        else:
            return self._run_result_outputs(run_result)

    def _session_run(self, inputs, trace_stats):
        if trace_stats:
            return self._session_run_with_stats(inputs)
        else:
            return self._session_run_default(inputs)

    def _session_run_with_stats(self, inputs):
        import tensorflow as tf
        meta = tf.RunMetadata()
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        output = self._session.run(
            self._output_tensors,
            inputs,
            options=run_options,
            run_metadata=meta)
        self._update_stats(meta)
        return output

    def _update_stats(self, meta):
        import tensorflow as tf
        ss = tf.contrib.stat_summarizer.NewStatSummarizer(
            self._session.graph_def.SerializeToString())
        ss.ProcessStepStatsStr(meta.step_stats.SerializeToString())
        stats_str = ss.GetOutputString()
        parse_and_apply_stats(stats_str, self._stats)

    def _session_run_default(self, inputs):
        return self._session.run(self._output_tensors, feed_dict=inputs)

    def _inputs_feed_dict(self, instances):
        return {
            tensor: [inst[key] for inst in instances]
            for key, tensor in self._input_tensor_map.items()
        }

    def _run_result_outputs(self, run_result):
        items = [{}] * len(run_result[0]) if run_result else []
        for key_index, values in enumerate(run_result):
            key = self._output_keys[key_index]
            for item_index, value in enumerate(values):
                items[item_index][key] = value.tolist()
        return items

    def info(self):
        return {
            "path": self._serving_path,
            "inputs": self._inputs_info(),
            "outputs": self._outputs_info()
        }

    def _inputs_info(self):
        return {
            key: tensor_info(tensor)
            for key, tensor in self._input_tensor_map.items()
        }

    def _outputs_info(self):
        return {
            key: tensor_info(tensor)
            for key, tensor in zip(self._output_keys, self._output_tensors)
        }

    def stats(self):
        return self._stats.generate()

def parse_and_apply_stats(stats_str, stats):
    result = try_parse_v1_stats(stats_str)
    if result:
        stats.update(*result)
    else:
        result = try_parse_pre_v1_stats(stats_str)
        if result:
            stats.update(*result)
        else:
            script_warning("WARNING: cannot parse stats")

def try_parse_v1_stats(stats_str):
    m = re.search(
        "Timings \(microseconds\): count=([0-9]+) curr=([0-9]+)\n"
        "Memory \(bytes\): count=[0-9]+ curr=([0-9]+)",
        stats_str, re.DOTALL)
    return [int(group) for group in m.groups()] if m else None

def try_parse_pre_v1_stats(stats_str):
    m = re.search(
        "curr=([0-9]+) count=([0-9]+).*\n(.*) avg KB",
        stats_str, re.DOTALL)
    if m:
        time, count, mem_kb = m.groups()
        return int(count), int(time), int(float(mem_kb) * 1024)
    else:
        return None

active_model = Model()

def ensure_active_model_serving_path(path, req):
    try:
        active_model.ensure_serving_path(path)
    except ModelError, e:
        raise ErrorResponse(req, str(e))

# ===================================================================
# Load model
# ===================================================================

def handle_load_model(req):
    model_path = validate_model_path_args(req)
    ensure_active_model_serving_path(model_path, req)
    send_ok_response(req)

def validate_model_path_args(req):
    if len(req.args) != 1:
        raise ProtocolError("%s requires 1 argument" % req.name)
    return validate_model_path(req.args[0], req)

def validate_model_path(path, req):
    if not os.path.exists(path + ".meta"):
        raise ErrorResponse(req, "not found")
    return path

# ===================================================================
# Run model
# ===================================================================

def handle_run_model(req, trace_stats=False):
    model_path, instances = validate_run_model_args(req)
    ensure_active_model_serving_path(model_path, req)
    try:
        outputs = active_model.run(instances, trace_stats)
    except ValueError, e:
        send_error_response(req, [str(e)])
    except ModelError, e:
        send_error_response(req, [str(e)])
    else:
        send_ok_response(req, [json.dumps(outputs)])

def handle_run_model_with_stats(req):
    handle_run_model(req, trace_stats=True)

def validate_run_model_args(req):
    if len(req.args) != 2:
        raise ProtocolError("run-model requires 2 arguments")
    model_path = validate_model_path(req.args[0], req)
    try:
        instances = json.loads(req.args[1])
    except ValueError:
        raise ErrorResponse(req, "run request must be valid JSON")
    if type(instances) != list:
        raise ErrorResponse(req, "run request must be a list")
    return model_path, instances

# ===================================================================
# Model info
# ===================================================================

def handle_model_info(req):
    model_path = validate_model_path_args(req)
    ensure_active_model_serving_path(model_path, req)
    info = active_model.info()
    send_ok_response(req, [json.dumps(info)])

# ===================================================================
# Model stats
# ===================================================================

def handle_model_stats(req):
    model_path = validate_model_path_args(req)
    ensure_active_model_serving_path(model_path, req)
    stats = active_model.stats()
    send_ok_response(req, [json.dumps(stats)])

# ===================================================================
# Port loop
# ===================================================================

def port_loop():
    while True:
        req = read_request()
        if not req:
            break
        try:
            handle_request(req)
        except ErrorResponse, e:
            send_error_response(e.req, [e.msg])

# ===================================================================
# Run once (for debugging)
# ===================================================================

def run_once(model_path, input_paths):
    active_model.ensure_serving_path(model_path)
    inputs = load_inputs(input_paths)
    if not inputs:
        raise ValueError("no inputs")
    outputs = active_model.run(inputs, trace_stats=True)
    print active_model.stats()
    print json.dumps(outputs)

def load_inputs(paths):
    return [json.load(open(path, "r")) for path in paths]

# ===================================================================
# Test
# ===================================================================

def test():
    test_model_stats()
    print "All tests passed"

def test_model_stats():
    def generate_stats(batches):
        stats = ModelStats()
        for count, time, mem in batches:
            stats.update(count, time, mem)
        return stats.generate()

    def assert_equals(stats, name, expected):
        actual = stats[name]
        if actual != expected:
            script_error(
                "ERROR: %s was %s, expected %s"
                % (name, actual, expected))

    s1 = generate_stats([])
    assert_equals(s1, "last_batch_time_ms", None)
    assert_equals(s1, "average_batch_time_ms", None)
    assert_equals(s1, "predictions_per_second", None)
    assert_equals(s1, "last_memory_bytes", None)

    s2 = generate_stats([(1, 10000, 100 * 1024)])
    assert_equals(s2, "last_batch_time_ms", 10.0)
    assert_equals(s2, "average_batch_time_ms", 10.0)
    assert_equals(s2, "predictions_per_second", 100.0)
    assert_equals(s2, "last_memory_bytes", 102400)

    s3 = generate_stats([(1, 10000, 100 * 1024), (1, 10000, 200 * 1024)])
    assert_equals(s3, "last_batch_time_ms", 10.0)
    assert_equals(s3, "average_batch_time_ms", 10.0)
    assert_equals(s3, "predictions_per_second", 100.0)
    assert_equals(s3, "last_memory_bytes", 204800)

    s4 = generate_stats([(1, 10000, 200 * 1024), (1, 20000, 100 * 1024)])
    assert_equals(s4, "last_batch_time_ms", 20.0)
    assert_equals(s4, "average_batch_time_ms", 50000 / 3 / 1000)
    assert_equals(s4, "predictions_per_second", 1 / (50000 / 3 / 1000000))
    assert_equals(s4, "last_memory_bytes", 102400)

    s5 = generate_stats(
        [(1, 60000, 101 * 1024),
         (1, 50000, 102 * 1024),
         (1, 40000, 103 * 1024),
         (1, 30000, 104 * 1024),
         (1, 20000, 105 * 1024),
         (1, 10000, 106 * 1024)])
    assert_equals(s5, "last_batch_time_ms", 10.0)
    assert_equals(s5, "average_batch_time_ms", 350000 / 15 / 1000)
    assert_equals(s5, "predictions_per_second", 1 / (350000 / 15 / 1000000))
    assert_equals(s5, "last_memory_bytes", 108544)

# ===================================================================
# Main
# ===================================================================

def script_error(msg):
    STDERR.write(msg + "\n")
    sys.exit(1)

def script_warning(msg):
    STDERR.write(msg + "\n")

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("cmd", metavar="CMD", nargs="?",
                   help="optional command: run | test")
    p.add_argument("--model", metavar="PATH",
                   help="model path")
    p.add_argument("--inputs",  nargs="+", metavar="PATTERN",
                   help=("pattern to match input sources"))
    args = p.parse_args()
    if args.cmd == "run":
        if not args.model:
            script_error("--model required for run command")
        if not args.inputs:
            script_error("--inputs required for run command")
        run_once(args.model, args.inputs)
    elif args.cmd == "test":
        test()
    else:
        port_loop()
